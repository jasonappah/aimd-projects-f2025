# Configuration for Transformer model

data:
  csv_path: "data/glucose_timeseries_5000_24h.csv"
  window_size: 6  # hours
  train_size: 0.7
  val_size: 0.15
  test_size: 0.15
  random_state: 42

preprocessing:
  forward_fill: true
  add_missing_indicators: true
  normalize_per_person: false

features:
  rolling_windows: [1, 3, 6]
  rolling_cols: ["glucose_mg_dL"]

model:
  input_size: null  # auto-detect from features
  d_model: 128
  nhead: 8
  num_layers: 4
  dim_feedforward: 512
  num_classes: 2
  dropout: 0.1
  max_seq_len: 100
  multitask: false

training:
  batch_size: 32
  num_epochs: 100
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "adamw"
  loss: "focal"  # or "weighted_bce"
  focal_alpha: 1.0
  focal_gamma: 2.0
  class_weight_pos: null  # auto-calculate
  grad_clip: 1.0

scheduler:
  type: "cosine"  # cosine, step, plateau, onecycle
  T_max: 100
  eta_min: 1e-6

callbacks:
  early_stopping:
    patience: 10
    mode: "max"
    metric_name: "pr_auc"
  checkpoint:
    checkpoint_dir: "checkpoints/transformer"
    save_best: true
    save_last: true

device:
  override: null  # null = auto-detect

